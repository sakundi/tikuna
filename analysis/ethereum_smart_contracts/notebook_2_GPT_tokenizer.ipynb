{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPuIaYYJLjvJ"
   },
   "source": [
    "# Train a new tokenizer based on smart contract opcodes\n",
    "\n",
    "LLMs or other types of language models cannot accept direct words or characters in text as raw inputs. They usually need a preprocessing phase called tokenization to transform the text input into a form more suitable for numerical processing.\n",
    "\n",
    "Many pretraining models, such as GPT2, the one we use here, come with their own \"tokenizer\" that was already trained with natural language text, such as the English Wikipedia. However, since there is no available Tokenizer for Ethereum smart contract bytecode, we are going to train our own! Cool :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jfwnEMBRLjvT"
   },
   "outputs": [],
   "source": [
    "# First we call the necessary libraries\n",
    "# We are going to use HuggingFace as our AI framework\n",
    "# It really makes our life easier when dealing with LLMs\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from collections import defaultdict\n",
    "\n",
    "# Then we load the data we collected in our previous tutorial\n",
    "dataset = load_dataset(\"text\", data_files={\"train\": \"/data/forta/ethereum/text/pretraining/pretraining_train.csv\",\n",
    "                                           \"val\": \"/data/forta/ethereum/text/pretraining/pretraining_val.csv\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max = -1\n",
    "for row in dataset[\"train\"][\"text\"]:\n",
    "    length_of_the_messages = row.split(\" \")\n",
    "    max = len(length_of_the_messages) if len(length_of_the_messages) > max else max\n",
    "print(\"Max number of words = \", max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T29hShJ1LjvX"
   },
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    batch_size = 400\n",
    "    aux_dataset = dataset[\"train\"]\n",
    "    for start_idx in range(0, len(aux_dataset), batch_size):\n",
    "        samples = aux_dataset[start_idx : start_idx + batch_size]\n",
    "        yield samples[\"text\"]\n",
    "\n",
    "training_corpus = get_training_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2QDOpAR1LjvX"
   },
   "outputs": [],
   "source": [
    "# Just to visualize process we compare the current GPT2 tokenizer with the one we are going to train\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "print(old_tokenizer.tokenize(\"PUSH1 PUSH1 MSTORE PUSH1 CALLDATASIZE LT PUSH2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2 tokenizer training from scratch\n",
    "\n",
    "Here, we define a vocabulary size that we found after repeating the training process several times. We take the GPT2 tokenizer and train it on the newly collected bytecode text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UoMI6s6JLjvY"
   },
   "outputs": [],
   "source": [
    "vocab_size = 524\n",
    "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, vocab_size)\n",
    "# Print an example just to compare with the pretrained tokenizer\n",
    "print(old_tokenizer.tokenize(\"PUSH1 PUSH1 MSTORE PUSH1 CALLDATASIZE LT PUSH2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RoHgMflNLjvZ"
   },
   "outputs": [],
   "source": [
    "# Finally store the resulting tokenizer so we can use it later in our LLM pretraining process.\n",
    "tokenizer.save_pretrained(\"/data/forta/ethereum/tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our following notebook tutorial [Finetuning data collection](notebook_3_GPT_finetuning_data_collection.ipynb), we will describe how to collect the data that will be used later in our example finetuning tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Training a new tokenizer from an old one",
   "provenance": [
    {
     "file_id": "https://github.com/huggingface/notebooks/blob/master/course/en/chapter6/section2.ipynb",
     "timestamp": 1708376608320
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
