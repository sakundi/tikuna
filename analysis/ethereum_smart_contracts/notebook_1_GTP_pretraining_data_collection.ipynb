{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba979f32-67fc-4dc5-a71b-be84043a87b9",
   "metadata": {},
   "source": [
    "# Big Language Model training using Ethereum Smart Contracts\n",
    "\n",
    "# Introduction\n",
    "\n",
    "In this series of notebooks, you will learn how to collect Ethereum (And other L2 networks) Smart Contract data and then train a GPT2 Language Model. The goal is to build a pretraining model that can later be used for specific AI applications. For instance, we will use the pre-trained model to fine-tune another model capable of classifying smart contracts as normal or as \"malicious,\" or in other words, to detect smart contracts that could be used for a cyberattack, even before they can do any harm. Please take a look at this repo to see another way to accomplish this same task by using machine learning algorithms: [Malicious Smart Contract ML](https://github.com/forta-network/starter-kits/tree/main/malicious-smart-contract-ml-py)\n",
    "\n",
    "# Smart Contract Training Dataset Collection\n",
    "\n",
    "First, we start by collecting the required data to train our LLM. We use general smart contract data, as well as data that has already been classified as trusted and data that has been collected from previous on-chain attacks. This notebook collects smart contract bytecode and decompiled opcodes for normal and malicious contract classification. Pretraining contracts are gathered from Zettablock and malicious contracts from [Forta Network's labeled datasets GitHub repo](https://github.com/forta-network/labelled-datasets).\n",
    "\n",
    "As a note, our goal in these tutorials is not to train the LLM to be able to reproduce or generate Solidity smart contract code, but to use bytecode that can be used in other kind of task such as classification.\n",
    "\n",
    "As another note, if you find something wrong or not clear in these tutorials, please report an issue here: [Tikuna Issues](https://github.com/edenia/tikuna/issues)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599dd826-f2b4-416c-b256-8eef021287a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by calling the required libraries\n",
    "\n",
    "import logging\n",
    "import pickle\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "from evmdasm import EvmBytecode\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from web3 import Web3\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# We use dotenv for environment variable configuration, so you need a .env file\n",
    "# with the secret variables such as ZETTABLOCK_API_KEY=<YOUR_API_KEY>\n",
    "# Load secrets\n",
    "dotenv_path = '../.env'\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "tqdm.pandas()\n",
    "# disable warning logs from evmdasm tool\n",
    "logging.getLogger(\"evmdasm\").setLevel(logging.CRITICAL)\n",
    "\n",
    "# We define where our data is going to be located in the file system\n",
    "zettablock_data_file = \"/data/forta/ethereum/text/pretraining/zettablock_data\"\n",
    "processed_data_file = \"/data/forta/ethereum/text/pretraining/big_pretrain_data\"\n",
    "\n",
    "# In this tutorial we use Zetablock to collect smart contract data\n",
    "# You can get a free account and have your API key to be able to replicate this \n",
    "# tutorial (https://app.zettablock.com/v2/explore/projects)\n",
    "# COnfigure your Zettablock API key\n",
    "API_KEY = os.environ.get(\"ZETTABLOCK_API_KEY\")\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"content-type\": \"application/json\",\n",
    "    # credentials\n",
    "    \"X-API-KEY\": API_KEY\n",
    "}\n",
    "\n",
    "# Zettablock endpoint\n",
    "ZETTABLOCK_DATA_LAKE_ENDPOINT = os.environ.get(\"ZETTABLOCK_DATA_LAKE_ENDPOINT\")\n",
    "\n",
    "# Configure the blockchains we are interested in\n",
    "# We will collect data from Ethereum, Polygon and BSC\n",
    "blockchains = [\"ethereum_mainnet\", \"polygon_mainnet\", \"bsc_mainnet\"]\n",
    "\n",
    "# Final training and validation files\n",
    "train_file_path = \"/data/forta/ethereum/text/pretraining/pretraining_train.csv\"\n",
    "val_file_path = \"/data/forta/ethereum/text/pretraining/pretraining_val.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88781f8f",
   "metadata": {},
   "source": [
    "# Collect smart contract data\n",
    "\n",
    "The Zetablock API allows us to download data from different Ethereum blockchains and use SQL-like queries to specify all the data we need for our training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62b6fa1-3333-4422-8b15-eb80b0206460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken from Zettablock tutorials\n",
    "# check response until success or failed is returned\n",
    "def get_response(queryrun_id):\n",
    "    import time\n",
    "    i = 1\n",
    "    queryrun_status_endpoint = f'https://api.zettablock.com/api/v1/queryruns/{queryrun_id}/status'\n",
    "    while True:\n",
    "        res = requests.get(queryrun_status_endpoint, headers=headers)\n",
    "        state = json.loads(res.text)['state']\n",
    "        if state == 'SUCCEEDED' or state == 'FAILED':\n",
    "            return state\n",
    "        time.sleep(i)\n",
    "        i += 1\n",
    "\n",
    "def download_file(url: str, local_file: str, headers=None, params=None):\n",
    "    resp = requests.get(url, stream=True, headers=headers, params=params)\n",
    "    total = int(resp.headers.get('content-length', 0))\n",
    "    with open(local_file, 'ab') as file, tqdm(\n",
    "        desc=local_file,\n",
    "        total=total,\n",
    "        unit='iB',\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as bar:\n",
    "        for data in resp.iter_content(chunk_size=1024):\n",
    "            size = file.write(data)\n",
    "            bar.update(size)\n",
    "\n",
    "def call_zettablock_api(query_text, blockchain):\n",
    "    # Get Smart Contract Data from Zettablock for several blockchains\n",
    "    query = {\"query\": query_text, \"resultCacheExpireMillis\": 86400000}\n",
    "    \n",
    "    # Create a query with SQL statement, and get query id\n",
    "    res = requests.post(data_lake_query_endpoint, headers=headers, data=json.dumps(query))\n",
    "    print(res.text)\n",
    "    \n",
    "    # Trigger the query by query id, and get queryrun id\n",
    "    query_id = res.json()['id']\n",
    "    data_lake_submission_endpoints = f'https://api.zettablock.com/api/v1/queries/{query_id}/trigger'\n",
    "    res = requests.post(data_lake_submission_endpoints, headers=headers, data='{}')\n",
    "    \n",
    "    # Check status using queryrun id\n",
    "    queryrun_id = res.json()['queryrunId']\n",
    "    \n",
    "    if get_response(queryrun_id) == 'SUCCEEDED':\n",
    "        # Fetch result from queryrun id\n",
    "        params = {'includeColumnName': 'true'}\n",
    "        queryrun_result_endpoint = f'https://api.zettablock.com/api/v1/stream/queryruns/{queryrun_id}/result'\n",
    "        # if the result is huge, consider using stream and write to a file\n",
    "        download_file(queryrun_result_endpoint, zettablock_data_file+\"_\"+blockchain+\".csv\", headers=headers, params=params)\n",
    "    else:\n",
    "        print('query failed, please check status message for details')\n",
    "        print(res.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf35513",
   "metadata": {},
   "source": [
    "# Preprocess smart contract bytecode\n",
    "\n",
    "Since we want to train an LLM, a model suitable for natural language processing, we need to extract information from the Smart Contracts that can be analyzed as text. We decided to use the contracts' bytecode as the model input, but still, we need to preprocess it beforehand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73efe3ca-4665-4b2d-aff3-3a4ce62b1e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code provided by the Forta team\n",
    "# Decompile and dissasemble the smart contract bytecode\n",
    "def get_opcodes(creation_bytecode) -> str:\n",
    "    bytecode = creation_bytecode\n",
    "    if bytecode is None:\n",
    "        return ''\n",
    "\n",
    "    try:\n",
    "        opcodes = EvmBytecode(bytecode).disassemble()\n",
    "    except Exception:\n",
    "        return ''\n",
    "    \n",
    "    return \" \".join([str(op).strip() for op in opcodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe717dc-febf-48bb-ab9d-1957fa5477c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code provided by the Forta team\n",
    "# Filter opcodes to get the best features\n",
    "def get_exp_2_features(row):\n",
    "    creator = row['contract_creator']\n",
    "    opcodes = row['decompiled_opcodes'].split()\n",
    "    mask = '0xffffffffffffffffffffffffffffffffffffffff'\n",
    "    features = []\n",
    "    for i in range(len(opcodes)-1):\n",
    "        first = opcodes[i]\n",
    "        second = opcodes[i+1]\n",
    "        if not first.startswith('0x'):\n",
    "            token = first\n",
    "            if first.startswith('UNKNOWN') or first.startswith('INVALID'):\n",
    "                token = first.split('_')[0]\n",
    "            features.append(token)\n",
    "        elif first == 'PUSH4':\n",
    "            features.append(second)\n",
    "        elif first == 'PUSH20':\n",
    "            if second == creator:\n",
    "                features.append('creator')\n",
    "            elif second == mask:\n",
    "                features.append(mask)\n",
    "            else:\n",
    "                features.append('address')\n",
    "        elif first == 'PUSH32':\n",
    "            features.append(second)\n",
    "    return \" \".join(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03212838",
   "metadata": {},
   "source": [
    "# Create the queries to download the data\n",
    "\n",
    "Here, we actually create the SQL-like queries to download the data from Zetablock. We use smart contract data such as contract address, contract name, and bytecode, among others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cccb91-ceb9-4e98-bddf-3b59e3b6aca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create queries for the supported blockchains\n",
    "# They don't have all the same available data\n",
    "def get_query(blockchain):\n",
    "    query_text = \"\"\n",
    "    if blockchain == \"ethereum_mainnet\":\n",
    "        query_text = '''\n",
    "            SELECT contract.address as contract_address,\n",
    "                   contract.name as contract_name,\n",
    "                   contract.creator as contract_creator,\n",
    "                   tags.name as contract_tag_name, \n",
    "                   tags.type as contract_type,\n",
    "                   contract.code as contract_code\n",
    "            FROM {}.contracts contract LEFT JOIN {}.labels tags ON tags.address = contract.address\n",
    "            LIMIT 30000\n",
    "        '''.format(blockchain, blockchain)\n",
    "    elif blockchain == \"polygon_mainnet\":\n",
    "        query_text = '''\n",
    "            SELECT contracts.address as contract_address,\n",
    "                   mappings.contract_name as contract_name,\n",
    "                   contracts.creator_address as contract_creator,\n",
    "                   mappings.contract_category as contract_type,\n",
    "                   contracts.bytecode as contract_code\n",
    "            FROM polygon_mainnet.contract_creations contracts LEFT JOIN polygon_mainnet.contract_mappings mappings ON mappings.contract_address = contracts.address\n",
    "            LIMIT 30000\n",
    "        '''.format(blockchain, blockchain)\n",
    "    else:\n",
    "        query_text = '''\n",
    "            SELECT contracts.address as contract_address,\n",
    "                   contracts.creator_address as contract_creator,\n",
    "                   contracts.bytecode as contract_code\n",
    "            FROM bsc_mainnet.contract_creations contracts\n",
    "            LIMIT 30000\n",
    "        '''.format(blockchain, blockchain)\n",
    "    return query_text\n",
    "\n",
    "# Get contracts and filter the data\n",
    "def get_pretrain_contracts():\n",
    "    # Get data from Zettablock for 3 different EVM compatible blockchains\n",
    "    for blockchain in blockchains:\n",
    "        if not os.path.exists(zettablock_data_file+\"_\"+blockchain+\".csv\"):\n",
    "            print(\"Dowloading data from %s...\" % (blockchain))\n",
    "            call_zettablock_api(get_query(blockchain), blockchain)\n",
    "        if not os.path.exists(processed_data_file+\"_\"+blockchain+\".csv\"):\n",
    "            \"\"\"Collects contracts from Zettablock and its decompiled opcodes.\"\"\" \n",
    "            chunksize = 10 ** 6\n",
    "            print(\"Decompiling and extracting opcodes for blockchain %s:\" % blockchain )\n",
    "            with pd.read_csv(zettablock_data_file+\"_\"+blockchain+\".csv\", chunksize=chunksize) as contract_reader:\n",
    "                for contracts in contract_reader:\n",
    "                    contracts['decompiled_opcodes'] = contracts['contract_code'].progress_apply(get_opcodes)\n",
    "                    contracts = contracts[(contracts['decompiled_opcodes'].notna()) & (contracts['decompiled_opcodes'] != '')]\n",
    "                    contracts.drop_duplicates('contract_address', inplace=True)\n",
    "                    contracts['decompiled_opcodes'] = contracts.progress_apply(get_exp_2_features, axis=1)\n",
    "                    # Store data so we don't have to download it all the time\n",
    "                    contracts.to_csv(processed_data_file+\"_\"+blockchain+\".csv\", mode='a')\n",
    "        else:\n",
    "            print(\"%s already exists.\" % processed_data_file+\"_\"+blockchain+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b506eb",
   "metadata": {},
   "source": [
    "# Run the collection and preprocessing code\n",
    "\n",
    "Finally, run the functions created to collect the data from Zetablock and execute the preprocessing to store the data that we will use later in the next tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838b65f6-7238-400b-889c-ea3af72dbadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually run the collection code\n",
    "get_pretrain_contracts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf19d9ae-1de8-4069-a093-592deff4d192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally prepare data for pretraining phase\n",
    "pretraining_data = {}\n",
    "for blockchain in blockchains:\n",
    "    # Load data from disk\n",
    "    pretraining_data[blockchain] = pd.read_csv(processed_data_file+\"_\"+blockchain+\".csv\")\n",
    "\n",
    "# Concat data in the same pandas variable\n",
    "pretraining_data = pd.concat(list(pretraining_data.values()))\n",
    "# Suffle data so we have mixed and heterogeneos samples from all the blockchains\n",
    "print(pretraining_data.shape)\n",
    "pretraining_data = pretraining_data.sample(frac = 1)\n",
    "\n",
    "# Define the amount of data you want to use in the training\n",
    "# Consider your processing capabilities, for example if you have\n",
    "# only a CPU available or actually you have set of GPUs then you \n",
    "# can increase this value\n",
    "training_samples = 60000\n",
    "# Save the data to disk\n",
    "training_data = pretraining_data[:training_samples]\n",
    "validation_data = pretraining_data[training_samples:]\n",
    "training_data['decompiled_opcodes'].to_csv(train_file_path, sep='\\t', index=False)\n",
    "validation_data['decompiled_opcodes'].to_csv(val_file_path, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a26709e",
   "metadata": {},
   "source": [
    "In the following notebook tutorial, [Train a new tokenizer based on smart contract opcodes](notebook_2_GPT_tokenizer.ipynb), we will use the collected data to train a Tokenizer, a necessary step to proceed with the pretraining process for the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c33d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
